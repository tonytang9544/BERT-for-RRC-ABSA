{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'great', 'value', 'for', 'the', 'money', '.', 'We', 'purchased', 'this', 'as', 'a', 'back', 'up', 'computer', 'after', 'our', 'more', 'expensive', 'HP', 'needed', 'to', 'be', 'repaired', '.', 'This', 'is', 'a', 'great', 'computer', '.', 'We', 'have', \"n't\", 'had', 'any', 'problems', 'with', 'it', 'at', 'all', '.', 'The', 'body', 'is', 'a', 'bit', 'cheaply', 'made', 'so', 'it', 'will', 'be', 'interesting', 'to', 'see', 'how', 'long', 'it', 'holds', 'up', '.', 'Overall', 'though', ',', 'for', 'the', 'money', 'spent', 'it', \"'\", 's', 'a', 'great', 'deal', '.']\n",
      "78\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, TweetTokenizer\n",
    "\n",
    "text = \"This is a great value for the money . We purchased this as a back up computer after our more expensive HP needed to be repaired . This is a great computer . We have n't had any problems with it at all . The body is a bit cheaply made so it will be interesting to see how long it holds up . Overall though , for the money spent it 's a great deal .\"\n",
    "\n",
    "tokens = TweetTokenizer().tokenize(text)\n",
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'great', 'value', 'for', 'the', 'money', '.', 'We', 'purchased', 'this', 'as', 'a', 'back', 'up', 'computer', 'after', 'our', 'more', 'expensive', 'HP', 'needed', 'to', 'be', 'repaired', '.', 'This', 'is', 'a', 'great', 'computer', '.', 'We', 'have', \"n't\", 'had', 'any', 'problems', 'with', 'it', 'at', 'all', '.', 'The', 'body', 'is', 'a', 'bit', 'cheaply', 'made', 'so', 'it', 'will', 'be', 'interesting', 'to', 'see', 'how', 'long', 'it', 'holds', 'up', '.', 'Overall', 'though', ',', 'for', 'the', 'money', 'spent', 'it', \"'s\", 'a', 'great', 'deal', '.']\n",
      "77\n"
     ]
    }
   ],
   "source": [
    "def is_whitespace(c):\n",
    "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "paragraph_text = text\n",
    "doc_tokens = []\n",
    "char_to_word_offset = []\n",
    "prev_is_whitespace = True\n",
    "for c in paragraph_text:\n",
    "    if is_whitespace(c):\n",
    "        prev_is_whitespace = True\n",
    "    else:\n",
    "        if prev_is_whitespace:\n",
    "            doc_tokens.append(c)\n",
    "        else:\n",
    "            doc_tokens[-1] += c\n",
    "        prev_is_whitespace = False\n",
    "    char_to_word_offset.append(len(doc_tokens) - 1)\n",
    "\n",
    "print(doc_tokens)\n",
    "print(len(doc_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  2066,  2023,  4633,  1012,  2009,  2003, 11559,   999,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "torch.Size([1, 11, 768])\n",
      "torch.Size([1, 768])\n",
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0110,  0.0159,  0.0500,  ..., -0.3678,  0.2542,  0.4237],\n",
      "         [ 0.3790,  0.0734, -0.1440,  ..., -0.1316,  0.6123,  0.2099],\n",
      "         [-0.1150,  0.4740,  0.9893,  ...,  0.1482,  0.3360,  0.3301],\n",
      "         ...,\n",
      "         [-0.4032,  0.1886,  0.0884,  ...,  0.2425, -0.1337, -0.1080],\n",
      "         [ 0.0968, -0.4204,  0.0740,  ...,  0.6097,  0.2299, -0.2964],\n",
      "         [ 0.4490,  0.1954,  0.3692,  ...,  0.3948, -0.3804, -0.4034]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-9.3437e-01, -5.4492e-01, -9.7169e-01,  8.4760e-01,  8.5664e-01,\n",
      "         -1.9093e-01,  9.0413e-01,  3.5569e-01, -9.3135e-01, -9.9999e-01,\n",
      "         -5.7048e-01,  9.7747e-01,  9.8090e-01,  7.7271e-01,  9.6551e-01,\n",
      "         -7.9853e-01, -4.3243e-01, -7.1797e-01,  3.5273e-01, -5.0210e-01,\n",
      "          8.1665e-01,  1.0000e+00, -1.5502e-01,  4.3701e-01,  5.6980e-01,\n",
      "          9.9723e-01, -8.4279e-01,  9.5875e-01,  9.6129e-01,  7.6641e-01,\n",
      "         -7.9373e-01,  2.2884e-01, -9.9319e-01, -2.6732e-01, -9.8575e-01,\n",
      "         -9.9464e-01,  5.8054e-01, -7.5908e-01, -7.6249e-02,  1.4950e-01,\n",
      "         -9.4846e-01,  2.8775e-01,  9.9999e-01,  1.2826e-01,  5.6330e-01,\n",
      "         -3.7571e-01, -1.0000e+00,  3.3402e-01, -8.8963e-01,  9.7179e-01,\n",
      "          9.6105e-01,  9.3084e-01,  1.7099e-01,  6.2436e-01,  5.5615e-01,\n",
      "         -4.7534e-01, -6.1627e-02,  1.1790e-01, -3.1135e-01, -6.6030e-01,\n",
      "         -7.0513e-01,  4.7131e-01, -9.6700e-01, -9.2398e-01,  9.6080e-01,\n",
      "          9.2879e-01, -1.6306e-01, -4.2066e-01, -1.2332e-01, -6.6342e-02,\n",
      "          9.2236e-01,  2.7519e-01, -3.9023e-01, -9.1952e-01,  8.5118e-01,\n",
      "          3.6293e-01, -7.6120e-01,  1.0000e+00, -5.1450e-01, -9.8143e-01,\n",
      "          9.6447e-01,  8.9436e-01,  7.0420e-01, -5.8719e-01,  7.6095e-01,\n",
      "         -1.0000e+00,  6.7473e-01, -1.6818e-01, -9.9265e-01,  3.2685e-01,\n",
      "          6.9671e-01, -2.9895e-01,  9.2047e-01,  7.1905e-01, -7.2113e-01,\n",
      "         -6.2531e-01, -4.2094e-01, -9.5725e-01, -3.4296e-01, -4.5691e-01,\n",
      "          1.3096e-01, -2.9482e-01, -4.9764e-01, -4.2001e-01,  4.7089e-01,\n",
      "         -6.0685e-01, -6.2406e-01,  7.0124e-01,  4.0846e-01,  7.9644e-01,\n",
      "          5.6026e-01, -4.9301e-01,  5.5125e-01, -9.6386e-01,  6.3608e-01,\n",
      "         -4.0737e-01, -9.8987e-01, -7.2015e-01, -9.9199e-01,  7.1060e-01,\n",
      "         -5.3466e-01, -3.3876e-01,  9.7048e-01, -6.5162e-01,  4.9529e-01,\n",
      "         -2.2044e-01, -9.6415e-01, -1.0000e+00, -7.6261e-01, -7.2890e-01,\n",
      "         -4.1446e-01, -3.3376e-01, -9.7958e-01, -9.6875e-01,  7.3014e-01,\n",
      "          9.6019e-01,  3.2251e-01,  9.9998e-01, -3.6662e-01,  9.5281e-01,\n",
      "         -6.2701e-01, -7.5469e-01,  8.4492e-01, -5.1022e-01,  8.9533e-01,\n",
      "          2.0773e-01, -6.6570e-01,  2.0890e-01, -5.3021e-01,  5.3057e-01,\n",
      "         -8.2413e-01, -2.2848e-01, -9.2017e-01, -9.5914e-01, -4.6297e-01,\n",
      "          9.6567e-01, -7.7890e-01, -9.7039e-01, -3.3580e-01, -2.0612e-01,\n",
      "         -5.8450e-01,  8.4485e-01,  8.2178e-01,  4.4552e-01, -5.9294e-01,\n",
      "          5.2714e-01,  4.5033e-01,  6.1872e-01, -8.5034e-01, -1.6164e-01,\n",
      "          4.4291e-01, -4.4154e-01, -9.5713e-01, -9.8545e-01, -4.9599e-01,\n",
      "          6.1759e-01,  9.9253e-01,  7.9974e-01,  3.9310e-01,  9.1581e-01,\n",
      "         -2.8544e-01,  8.4813e-01, -9.6878e-01,  9.8537e-01, -3.3479e-01,\n",
      "          3.5250e-01, -7.6744e-01,  6.8472e-01, -8.7403e-01,  5.8724e-01,\n",
      "          8.3467e-01, -8.7121e-01, -8.3461e-01, -1.2952e-01, -5.4307e-01,\n",
      "         -4.7540e-01, -9.4858e-01,  4.4523e-01, -4.3058e-01, -4.3708e-01,\n",
      "         -2.1916e-01,  9.5959e-01,  9.8107e-01,  8.3969e-01,  5.5498e-01,\n",
      "          7.7849e-01, -9.3499e-01, -6.7473e-01,  1.1683e-01,  2.2912e-01,\n",
      "          3.0337e-01,  9.9515e-01, -8.5799e-01, -1.7728e-01, -9.5550e-01,\n",
      "         -9.8898e-01, -1.3195e-01, -9.2054e-01, -2.4049e-01, -7.9205e-01,\n",
      "          7.6530e-01, -5.7992e-01,  6.8492e-01,  5.5243e-01, -9.8526e-01,\n",
      "         -8.0930e-01,  5.2091e-01, -4.4983e-01,  5.3457e-01, -2.9661e-01,\n",
      "          9.4336e-01,  9.8501e-01, -7.4631e-01,  7.0673e-01,  9.5543e-01,\n",
      "         -9.8608e-01, -7.8931e-01,  8.2048e-01, -3.3759e-01,  8.9298e-01,\n",
      "         -7.4289e-01,  9.9269e-01,  9.6080e-01,  8.6090e-01, -9.3434e-01,\n",
      "         -9.0921e-01, -8.8988e-01, -8.5247e-01, -2.1229e-01,  4.2148e-01,\n",
      "          9.6118e-01,  7.5517e-01,  5.7129e-01,  1.1890e-01, -6.5566e-01,\n",
      "          9.9856e-01, -9.7840e-01, -9.5983e-01, -6.8288e-01, -4.4309e-01,\n",
      "         -9.8958e-01,  9.6518e-01,  4.4758e-01,  8.2211e-01, -5.9781e-01,\n",
      "         -7.8802e-01, -9.7227e-01,  9.0157e-01,  1.9992e-01,  9.8815e-01,\n",
      "         -5.6017e-01, -9.3844e-01, -7.8836e-01, -9.5152e-01, -1.4907e-01,\n",
      "         -2.7366e-01, -6.8488e-01, -1.1538e-01, -9.7180e-01,  5.9055e-01,\n",
      "          6.0112e-01,  6.1004e-01, -9.1931e-01,  9.9935e-01,  1.0000e+00,\n",
      "          9.8184e-01,  8.8912e-01,  9.2239e-01, -9.9999e-01, -8.6827e-01,\n",
      "          1.0000e+00, -9.9830e-01, -1.0000e+00, -9.3205e-01, -7.6851e-01,\n",
      "          4.5948e-01, -1.0000e+00, -2.8039e-01, -8.5103e-03, -9.3482e-01,\n",
      "          7.8232e-01,  9.7484e-01,  9.9466e-01, -1.0000e+00,  8.5841e-01,\n",
      "          9.5626e-01, -7.3915e-01,  9.8755e-01, -5.6976e-01,  9.6911e-01,\n",
      "          8.0790e-01,  6.1052e-01, -2.7248e-01,  4.6410e-01, -9.8413e-01,\n",
      "         -8.9076e-01, -7.7803e-01, -9.0850e-01,  9.9981e-01,  2.3955e-01,\n",
      "         -8.2369e-01, -9.2645e-01,  6.3204e-01, -2.8058e-01, -4.2102e-02,\n",
      "         -9.8000e-01, -3.5606e-01,  6.3258e-01,  8.7432e-01,  2.7574e-01,\n",
      "          4.8459e-01, -7.4268e-01,  3.4794e-01,  3.9201e-01,  2.9212e-01,\n",
      "          7.5097e-01, -9.3327e-01, -5.9165e-01, -1.3112e-01, -7.0664e-02,\n",
      "         -8.3908e-01, -9.7213e-01,  9.7301e-01, -5.3928e-01,  9.0385e-01,\n",
      "          1.0000e+00,  5.2295e-01, -9.1149e-01,  7.6370e-01,  3.9347e-01,\n",
      "         -7.5028e-01,  1.0000e+00,  9.3105e-01, -9.8379e-01, -7.0680e-01,\n",
      "          8.1286e-01, -7.1499e-01, -7.3602e-01,  9.9968e-01, -3.6126e-01,\n",
      "         -8.5041e-01, -7.4265e-01,  9.8434e-01, -9.9040e-01,  9.9930e-01,\n",
      "         -9.2178e-01, -9.7828e-01,  9.7220e-01,  9.5992e-01, -8.0890e-01,\n",
      "         -7.7316e-01,  2.2460e-01, -8.6630e-01,  4.1745e-01, -9.6312e-01,\n",
      "          7.9842e-01,  5.4819e-01, -3.4817e-02,  9.2933e-01, -7.8779e-01,\n",
      "         -6.9214e-01,  3.2407e-01, -6.8502e-01, -2.8669e-01,  9.8538e-01,\n",
      "          5.7769e-01, -3.5169e-01,  8.0977e-02, -3.8710e-01, -8.0234e-01,\n",
      "         -9.8120e-01,  7.2633e-01,  1.0000e+00, -5.2150e-01,  9.3291e-01,\n",
      "         -6.0707e-01, -5.2971e-02,  5.4790e-02,  6.8152e-01,  7.0502e-01,\n",
      "         -3.9055e-01, -9.1292e-01,  9.4118e-01, -9.7768e-01, -9.9112e-01,\n",
      "          8.1339e-01,  2.9103e-01, -3.7433e-01,  1.0000e+00,  5.9624e-01,\n",
      "          2.6889e-01,  5.9870e-01,  9.9434e-01, -7.3336e-02,  5.9800e-01,\n",
      "          9.6111e-01,  9.8611e-01, -3.1330e-01,  6.7628e-01,  8.8014e-01,\n",
      "         -9.6345e-01, -4.1651e-01, -7.4733e-01,  9.6829e-02, -9.5507e-01,\n",
      "          6.8627e-02, -9.7271e-01,  9.7593e-01,  9.6139e-01,  5.1506e-01,\n",
      "          3.7809e-01,  8.9095e-01,  1.0000e+00, -8.6307e-01,  6.7404e-01,\n",
      "         -2.4172e-03,  7.8793e-01, -9.9999e-01, -8.8428e-01, -4.9687e-01,\n",
      "         -5.8452e-02, -9.3847e-01, -4.7998e-01,  3.7837e-01, -9.7458e-01,\n",
      "          9.1525e-01,  8.2940e-01, -9.9528e-01, -9.9112e-01, -6.2206e-01,\n",
      "          7.9989e-01,  1.3327e-01, -9.9682e-01, -7.9271e-01, -5.9400e-01,\n",
      "          7.8201e-01, -4.4454e-01, -9.5159e-01, -3.9336e-01, -5.0854e-01,\n",
      "          4.8125e-01, -3.1761e-01,  6.8523e-01,  9.3237e-01,  5.9123e-01,\n",
      "         -8.8575e-01, -4.4704e-01, -1.0085e-01, -8.2633e-01,  8.6393e-01,\n",
      "         -8.5856e-01, -9.8476e-01, -2.5277e-01,  1.0000e+00, -6.9027e-01,\n",
      "          9.7224e-01,  7.8535e-01,  8.0143e-01, -2.1598e-01,  2.5483e-01,\n",
      "          9.8582e-01,  4.4248e-01, -9.1217e-01, -9.4623e-01, -2.5905e-01,\n",
      "         -5.2968e-01,  7.5469e-01,  8.3166e-01,  8.7575e-01,  8.6038e-01,\n",
      "          9.0648e-01,  2.2711e-01, -4.4201e-02,  5.7797e-04,  9.9980e-01,\n",
      "         -2.5091e-01, -4.6656e-01, -5.8701e-01, -1.1230e-01, -4.8535e-01,\n",
      "         -4.1852e-03,  1.0000e+00,  3.3548e-01,  6.4364e-01, -9.9075e-01,\n",
      "         -9.5246e-01, -9.0385e-01,  1.0000e+00,  8.9270e-01, -7.7222e-01,\n",
      "          8.0830e-01,  7.9655e-01, -1.4672e-01,  8.7031e-01, -2.9943e-01,\n",
      "         -3.9060e-01,  4.0516e-01,  2.3331e-01,  9.5970e-01, -7.0383e-01,\n",
      "         -9.7597e-01, -4.9391e-01,  5.3707e-01, -9.8085e-01,  1.0000e+00,\n",
      "         -6.6289e-01, -3.2078e-01, -4.8688e-01, -4.6015e-01, -2.8618e-01,\n",
      "          1.9596e-03, -9.8786e-01, -2.2852e-01,  3.4761e-01,  9.6792e-01,\n",
      "          3.1970e-01, -6.8731e-01, -9.1242e-01,  9.0926e-01,  8.6173e-01,\n",
      "         -9.7034e-01, -9.4245e-01,  9.7062e-01, -9.8848e-01,  8.3242e-01,\n",
      "          1.0000e+00,  4.0567e-01,  2.0047e-01,  2.8390e-01, -6.0082e-01,\n",
      "          5.0512e-01, -6.8491e-01,  7.2955e-01, -9.6533e-01, -5.5803e-01,\n",
      "         -2.7593e-01,  3.4453e-01, -2.0836e-01, -6.2040e-01,  6.4476e-01,\n",
      "          2.2179e-01, -6.6017e-01, -7.3148e-01, -1.2116e-01,  5.5932e-01,\n",
      "          8.8970e-01, -2.5005e-01, -2.4800e-01,  5.1141e-02, -2.1178e-01,\n",
      "         -9.5126e-01, -4.6986e-01, -5.3942e-01, -1.0000e+00,  7.7345e-01,\n",
      "         -1.0000e+00,  7.4282e-01,  4.3375e-01, -2.6306e-01,  8.7995e-01,\n",
      "          8.1047e-01,  7.5983e-01, -7.9202e-01, -9.5027e-01,  2.2548e-01,\n",
      "          7.9406e-01, -4.1612e-01, -4.8169e-01, -7.2288e-01,  5.0056e-01,\n",
      "         -1.7024e-01,  4.1134e-01, -7.7068e-01,  7.9925e-01, -4.1837e-01,\n",
      "          1.0000e+00,  2.5817e-01, -8.0531e-01, -9.8692e-01,  2.3897e-01,\n",
      "         -3.0827e-01,  1.0000e+00, -9.2090e-01, -9.5661e-01,  5.8878e-01,\n",
      "         -8.0567e-01, -8.7942e-01,  4.5079e-01,  1.3061e-02, -8.3985e-01,\n",
      "         -9.7689e-01,  9.6633e-01,  8.6079e-01, -7.2321e-01,  6.6226e-01,\n",
      "         -4.2693e-01, -7.1546e-01, -8.0471e-02,  9.4895e-01,  9.9078e-01,\n",
      "          6.6800e-01,  9.3251e-01, -5.2243e-01, -5.4351e-01,  9.8012e-01,\n",
      "          2.7535e-01,  4.2911e-01,  1.6691e-01,  1.0000e+00,  4.1914e-01,\n",
      "         -9.2808e-01, -2.2172e-01, -9.8635e-01, -2.7426e-01, -9.6600e-01,\n",
      "          3.7802e-01,  3.8001e-01,  9.5341e-01, -3.5087e-01,  9.5981e-01,\n",
      "         -9.2798e-01,  7.8102e-02, -7.8685e-01, -7.2595e-01,  4.8550e-01,\n",
      "         -9.5012e-01, -9.8608e-01, -9.8346e-01,  7.8464e-01, -5.3064e-01,\n",
      "         -1.0536e-01,  3.1540e-01,  9.9147e-02,  5.4748e-01,  5.9223e-01,\n",
      "         -1.0000e+00,  9.5559e-01,  4.9912e-01,  9.7141e-01,  9.6533e-01,\n",
      "          8.9480e-01,  6.6177e-01,  3.7002e-01, -9.8658e-01, -9.8782e-01,\n",
      "         -4.3709e-01, -2.3846e-01,  7.9612e-01,  7.9348e-01,  9.3867e-01,\n",
      "          4.7122e-01, -5.4704e-01, -6.9179e-01, -8.8436e-01, -9.5360e-01,\n",
      "         -9.9520e-01,  4.4539e-01, -7.7145e-01, -9.6344e-01,  9.7676e-01,\n",
      "          1.1794e-01, -1.9647e-01, -4.3065e-01, -9.4998e-01,  9.3346e-01,\n",
      "          8.4506e-01,  3.4331e-01,  1.6937e-01,  4.6554e-01,  8.8380e-01,\n",
      "          9.5705e-01,  9.7918e-01, -9.3984e-01,  7.7737e-01, -8.6685e-01,\n",
      "          6.3316e-01,  8.8841e-01, -9.4312e-01,  2.3144e-01,  6.3305e-01,\n",
      "         -4.9694e-01,  3.0873e-01, -2.9353e-01, -9.7373e-01,  5.4687e-01,\n",
      "         -3.6138e-01,  7.4321e-01, -5.0815e-01, -6.6913e-02, -5.3476e-01,\n",
      "         -2.1476e-01, -7.1373e-01, -8.0790e-01,  7.3440e-01,  5.4136e-01,\n",
      "          9.1636e-01,  9.6171e-01, -1.3226e-01, -7.3192e-01, -2.5271e-01,\n",
      "         -8.9032e-01, -9.3376e-01,  9.3182e-01, -1.7749e-01, -5.9100e-01,\n",
      "          8.8898e-01,  1.6131e-01,  9.1896e-01,  3.9999e-01, -5.2709e-01,\n",
      "         -4.4820e-01, -7.8642e-01,  9.1391e-01, -6.1762e-01, -6.9976e-01,\n",
      "         -7.2085e-01,  8.4509e-01,  3.8178e-01,  1.0000e+00, -9.0377e-01,\n",
      "         -9.6499e-01, -6.2521e-01, -4.9664e-01,  5.3833e-01, -6.4074e-01,\n",
      "         -1.0000e+00,  5.2145e-01, -7.5954e-01,  8.6692e-01, -8.7881e-01,\n",
      "          9.4513e-01, -7.4882e-01, -9.8745e-01, -3.8535e-01,  7.3237e-01,\n",
      "          8.9705e-01, -5.3801e-01, -7.0813e-01,  6.8983e-01, -3.2387e-01,\n",
      "          9.9398e-01,  8.7809e-01, -8.9505e-02, -6.2502e-02,  7.8651e-01,\n",
      "         -9.1910e-01, -7.9276e-01,  9.6011e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "sentence = \"I like this weather. It is sunny!\"\n",
    "\n",
    "inputs = tokenizer(sentence, add_special_tokens=True, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "print(inputs)\n",
    "output = bert(**inputs)\n",
    "\n",
    "print(output.last_hidden_state.shape)\n",
    "print(output.pooler_output.shape)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules import MultiheadAttention\n",
    "\n",
    "a = MultiheadAttention(embed_dim=768, num_heads=8)\n",
    "\n",
    "print(a())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2129,  2003,  1996,  3643,  1029,   102,     0,     0],\n",
      "        [  101,  2079,  2017,  2066,  2049,  4773, 28727,  1029,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor([[0.4950, 0.5052, 0.5062, 0.5072, 0.5011, 0.4976, 0.5045, 0.5012, 0.5056,\n",
      "         0.4956, 0.4978, 0.5083, 0.5086, 0.5074, 0.5092, 0.5047, 0.5090, 0.5165,\n",
      "         0.4962, 0.4971, 0.4996, 0.5025, 0.5165, 0.5003, 0.5047, 0.4988, 0.5083,\n",
      "         0.4947, 0.5025, 0.5039, 0.5055, 0.4976, 0.5125, 0.5240, 0.4954, 0.4941,\n",
      "         0.5004, 0.4888, 0.4940, 0.4903, 0.4841, 0.5006, 0.5014, 0.5039, 0.4866,\n",
      "         0.4895, 0.4901, 0.5056, 0.5078, 0.5067, 0.5084, 0.5139, 0.5137, 0.5177,\n",
      "         0.5220, 0.5068, 0.4982, 0.5006, 0.4935, 0.4936, 0.4995, 0.5022, 0.4995,\n",
      "         0.4905, 0.5009, 0.4826, 0.4961, 0.5281, 0.5068, 0.5129, 0.5122, 0.4959,\n",
      "         0.4881, 0.4964, 0.4941, 0.4983, 0.4987, 0.4972, 0.4997, 0.4958, 0.4930,\n",
      "         0.5210, 0.5058, 0.4961, 0.5021, 0.5001, 0.4967, 0.5015, 0.4971, 0.4999,\n",
      "         0.4977, 0.5011, 0.5059, 0.5009, 0.4992, 0.4974, 0.5021, 0.5067, 0.4981,\n",
      "         0.4956, 0.4991, 0.5064, 0.5119, 0.5026, 0.4957, 0.4931, 0.5060, 0.5132,\n",
      "         0.5144, 0.5161, 0.5150, 0.5066, 0.5151, 0.4947, 0.5002, 0.5027, 0.4968,\n",
      "         0.5038, 0.4972, 0.4945, 0.4969, 0.4990, 0.4983, 0.5078, 0.4984, 0.5077,\n",
      "         0.5158, 0.4964, 0.4957, 0.4990, 0.5015, 0.5104, 0.4967, 0.4957, 0.4963,\n",
      "         0.5142, 0.5093, 0.5079, 0.5101, 0.4993, 0.5119, 0.5142, 0.4951, 0.4955,\n",
      "         0.5006, 0.4951, 0.5011, 0.4998, 0.4951, 0.4940, 0.4949, 0.4962, 0.5010,\n",
      "         0.5041, 0.5059, 0.5007, 0.5008, 0.5099, 0.5078, 0.5036, 0.4966, 0.4966,\n",
      "         0.5024, 0.5129, 0.5055, 0.4995, 0.4930, 0.5033, 0.5130, 0.5141, 0.5130,\n",
      "         0.5158, 0.5069, 0.5160, 0.4958, 0.5064, 0.5075, 0.5093, 0.5036, 0.4978,\n",
      "         0.5047, 0.5065, 0.5064, 0.4946, 0.4969, 0.4990, 0.5041, 0.5031, 0.5028,\n",
      "         0.5044, 0.4927, 0.5066, 0.5015, 0.4975, 0.5052, 0.4988, 0.4959, 0.4936,\n",
      "         0.5007, 0.4994, 0.5005, 0.5036, 0.4995, 0.5109, 0.5048, 0.4990, 0.5057,\n",
      "         0.5137, 0.5097, 0.4912, 0.5097, 0.5050, 0.4959, 0.5074, 0.5039, 0.4959,\n",
      "         0.4975, 0.5032, 0.5008, 0.5025, 0.5056, 0.5053, 0.4954, 0.4948, 0.4945,\n",
      "         0.4974, 0.4976, 0.4942, 0.4947, 0.4958, 0.4943, 0.4938, 0.5019, 0.4965,\n",
      "         0.4993, 0.5008, 0.5032, 0.5129, 0.5051, 0.5036, 0.5118, 0.5091, 0.5135,\n",
      "         0.5126, 0.5080, 0.5037, 0.5140, 0.5016, 0.5149, 0.5154, 0.5055, 0.5081,\n",
      "         0.5136, 0.5070, 0.5132, 0.5070, 0.4940, 0.4981, 0.4930, 0.4921, 0.4946,\n",
      "         0.4871, 0.4917, 0.4923, 0.4947, 0.5144, 0.5033, 0.5112, 0.5150, 0.4982,\n",
      "         0.5121, 0.4981, 0.4967, 0.4953, 0.4943, 0.5087, 0.5083, 0.5051, 0.5045,\n",
      "         0.5064, 0.5100, 0.5144, 0.5108],\n",
      "        [0.4966, 0.5077, 0.4912, 0.4914, 0.4916, 0.4957, 0.5079, 0.4900, 0.4894,\n",
      "         0.4914, 0.5005, 0.5011, 0.4955, 0.5079, 0.4951, 0.4925, 0.4825, 0.5039,\n",
      "         0.5071, 0.4959, 0.4930, 0.4924, 0.4874, 0.4966, 0.4891, 0.4834, 0.4872,\n",
      "         0.4874, 0.4895, 0.4981, 0.4886, 0.5055, 0.5010, 0.4932, 0.5012, 0.4957,\n",
      "         0.4996, 0.4952, 0.5063, 0.4895, 0.4859, 0.4969, 0.4923, 0.4996, 0.4941,\n",
      "         0.5013, 0.5144, 0.5021, 0.5057, 0.5056, 0.5110, 0.5013, 0.4973, 0.4995,\n",
      "         0.4868, 0.4983, 0.4984, 0.4998, 0.5036, 0.5014, 0.4904, 0.4995, 0.4992,\n",
      "         0.4991, 0.4922, 0.5062, 0.4994, 0.5003, 0.4978, 0.4948, 0.4914, 0.4891,\n",
      "         0.4856, 0.4871, 0.4929, 0.4944, 0.4955, 0.4868, 0.4950, 0.5062, 0.5105,\n",
      "         0.5002, 0.5031, 0.5071, 0.4960, 0.5026, 0.5062, 0.5052, 0.5033, 0.4966,\n",
      "         0.5095, 0.5021, 0.4945, 0.4945, 0.5003, 0.4979, 0.4949, 0.5030, 0.5009,\n",
      "         0.5005, 0.5002, 0.5014, 0.4985, 0.5016, 0.4899, 0.5112, 0.5153, 0.5182,\n",
      "         0.5049, 0.4995, 0.4948, 0.4903, 0.5082, 0.4990, 0.4989, 0.5047, 0.5020,\n",
      "         0.5122, 0.5036, 0.5035, 0.4994, 0.4855, 0.5087, 0.5098, 0.4994, 0.4993,\n",
      "         0.5023, 0.4998, 0.4970, 0.4930, 0.5031, 0.5003, 0.5016, 0.5028, 0.4983,\n",
      "         0.5086, 0.4868, 0.4966, 0.4996, 0.5038, 0.5101, 0.5085, 0.5086, 0.5037,\n",
      "         0.5193, 0.5260, 0.5181, 0.5105, 0.5095, 0.4994, 0.4905, 0.4965, 0.5031,\n",
      "         0.5056, 0.5045, 0.5005, 0.5057, 0.5151, 0.5078, 0.5044, 0.5085, 0.5174,\n",
      "         0.5063, 0.4950, 0.4956, 0.5013, 0.4878, 0.5013, 0.4956, 0.5098, 0.4985,\n",
      "         0.4949, 0.4912, 0.4873, 0.5094, 0.5077, 0.5065, 0.5061, 0.4968, 0.4899,\n",
      "         0.4995, 0.4988, 0.5051, 0.5112, 0.5073, 0.5090, 0.4873, 0.5019, 0.5018,\n",
      "         0.4946, 0.4934, 0.4923, 0.4983, 0.4922, 0.4954, 0.5012, 0.5125, 0.5090,\n",
      "         0.5092, 0.5111, 0.5029, 0.5013, 0.5082, 0.5001, 0.4914, 0.5127, 0.5122,\n",
      "         0.4994, 0.5002, 0.5032, 0.5091, 0.5079, 0.5111, 0.4969, 0.5048, 0.5045,\n",
      "         0.4865, 0.4979, 0.5060, 0.5030, 0.5016, 0.5018, 0.5033, 0.5086, 0.5128,\n",
      "         0.5005, 0.5147, 0.5123, 0.5034, 0.5096, 0.4995, 0.5041, 0.4986, 0.5084,\n",
      "         0.5038, 0.5163, 0.5096, 0.5083, 0.4988, 0.4986, 0.5030, 0.4883, 0.5061,\n",
      "         0.5045, 0.4969, 0.4953, 0.4831, 0.5049, 0.4866, 0.5175, 0.5072, 0.5041,\n",
      "         0.5116, 0.5049, 0.5004, 0.4962, 0.4909, 0.5069, 0.4914, 0.5007, 0.5051,\n",
      "         0.4843, 0.5080, 0.4992, 0.5003, 0.5076, 0.5039, 0.5062, 0.5045, 0.4966,\n",
      "         0.5006, 0.5022, 0.4872, 0.4999, 0.4991, 0.5058, 0.4993, 0.5000, 0.4989,\n",
      "         0.4984, 0.4911, 0.4976, 0.4996]], grad_fn=<SigmoidBackward0>)\n",
      "torch.Size([2, 283])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "\n",
    "class DualEncoder(torch.nn.Module):\n",
    "    def __init__(self, model_name, model_hidden_dim, attn_heads = 8):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.q_linear = torch.nn.Linear(model_hidden_dim, model_hidden_dim)\n",
    "        self.k_linear = torch.nn.Linear(model_hidden_dim, model_hidden_dim)\n",
    "\n",
    "        # self.attn = MultiheadAttention(embed_dim=model_hidden_dim, num_heads=attn_heads)\n",
    "        # self.linear = torch.nn.Linear(in_features=model_hidden_dim, out_features=1)\n",
    "\n",
    "    def forward(self, question, context):\n",
    "        q_e = F.normalize(self.q_linear(self.bert(**question).pooler_output), dim=-1)\n",
    "        k_e = F.normalize(self.k_linear(self.bert(**context).last_hidden_state), dim=-1)\n",
    "\n",
    "        return F.sigmoid(torch.einsum('bij,bj->bi', k_e, q_e))\n",
    "\n",
    "\n",
    "        # q_e = self.bert(**question).last_hidden_state\n",
    "        # k_e = v_e = self.bert(**context).last_hidden_state\n",
    "        # attn_output = self.attn(q_e, k_e, v_e)\n",
    "\n",
    "        # return F.sigmoid(self.linear(attn_output))\n",
    "        \n",
    "    \n",
    "\n",
    "sentence = [\n",
    "    \"This is a great value for the money . We purchased this as a back up computer after our more expensive HP needed to be repaired . This is a great computer . We have n't had any problems with it at all . The body is a bit cheaply made so it will be interesting to see how long it holds up . Overall though , for the money spent it 's a great deal .\",\n",
    "    \"Right out of the box , this little netbook did everything I asked of it , including streaming the everyday video you 're bound to encounter checking mail and websites ( my biggest complaint previously ) . It even has a great webcam , and Skype works very well . The fact that you can spend over $ 100 on just a webcam underscores the value of this machine . The Windows 7 Starter is , in my opinion , a great way to think about using your netbook : basics , basics , basics . I wiped nearly everything off of it , installed OpenOffice and Firefox , and I am operating an incredibly efficient and useful machine for a great price . This netbook is a perfect supplementary computer to another laptop or desktop ( my wife and I have another laptop ) , or if you are a user who uses the computer for simple tasks . I use this for my tutoring business , and since I 'm always bouncing from student to student , it is ideal for portability and battery life ( yes , it gets the 8 hours as advertised ! ) . Finally , I should note that I took the 2GB RAM stick from my old EeePC and installed it before I even powered on for the first time . ASUS has done an outstanding job of evolving their netbooks , and I would recommend this to anyone who both understands their needs and how netbooks can fit them .\", \n",
    "]\n",
    "\n",
    "question = [\n",
    "    \"how is the value ?\",\n",
    "    \"do you like its webcam ?\"\n",
    "\n",
    "]\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "q_t = tokenizer(question, return_tensors=\"pt\", add_special_tokens=True, padding=True, truncation=True)\n",
    "c_t = tokenizer(sentence, return_tensors=\"pt\", add_special_tokens=True, padding=True, truncation=True)\n",
    "\n",
    "print(q_t)\n",
    "\n",
    "answer = DualEncoder(model_name, 768)(q_t, c_t)\n",
    "print(answer)\n",
    "print(answer.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2129, 2003,  ...,    0,    0,    0],\n",
      "        [ 101, 2129, 2003,  ...,    0,    0,    0],\n",
      "        [ 101, 2079, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 2515, 1996,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "tensor([[0.4964, 0.4879, 0.4915,  ..., 0.4938, 0.4939, 0.4963],\n",
      "        [0.4971, 0.4884, 0.4900,  ..., 0.4927, 0.4939, 0.4976],\n",
      "        [0.4963, 0.4952, 0.5104,  ..., 0.4911, 0.4964, 0.4837],\n",
      "        [0.4966, 0.4960, 0.5094,  ..., 0.4904, 0.4963, 0.4844]],\n",
      "       grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "def read_json_examples(input_file):\n",
    "    with open(input_file, \"r\", encoding='utf-8') as reader:\n",
    "        input_data = json.load(reader)[\"data\"]\n",
    "    \n",
    "    questions = []\n",
    "    contexts = []\n",
    "    question_ids = []\n",
    "    answer_texts = []\n",
    "    start_positions = []\n",
    "\n",
    "    for entry in input_data:\n",
    "        for paragraph in entry[\"paragraphs\"]:\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                contexts.append(paragraph[\"context\"])\n",
    "                questions.append(qa[\"question\"])\n",
    "                question_ids.append(qa[\"id\"])\n",
    "                answer_texts.append(qa[\"answers\"][0][\"text\"])\n",
    "                start_positions.append(qa[\"answers\"][0][\"answer_start\"])\n",
    "\n",
    "    return contexts, questions, question_ids, answer_texts, start_positions\n",
    "\n",
    "\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, questions, contexts, answers, answer_starts, tokenizer, question_ids, max_length=512):\n",
    "        self.questions = questions\n",
    "        self.contexts = contexts\n",
    "        self.answers = answers\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.answer_starts = answer_starts\n",
    "        self.question_ids = question_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        context = self.contexts[idx]\n",
    "        answer = self.answers[idx]\n",
    "        question_id = self.question_ids[idx]\n",
    "\n",
    "        # Tokenize the input pair (question, context)\n",
    "        q_encode = self.tokenizer(\n",
    "            question,\n",
    "            add_special_tokens=True,   # Add [CLS] and [SEP] tokens\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',      # Pad to max_length\n",
    "            truncation=True,           # Truncate if too long\n",
    "            return_tensors='pt'        # Return PyTorch tensors\n",
    "        )\n",
    "        q_encode = {k:v.squeeze() for k, v in q_encode.items()}\n",
    "        # print(q_encode)\n",
    "\n",
    "        c_encode = self.tokenizer(\n",
    "            context,\n",
    "            add_special_tokens=True,   # Add [CLS] and [SEP] tokens\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',      # Pad to max_length\n",
    "            truncation=True,           # Truncate if too long\n",
    "            return_tensors='pt'        # Return PyTorch tensors\n",
    "        )\n",
    "\n",
    "        c_encode = {k:v.squeeze() for k, v in c_encode.items()}\n",
    "\n",
    "\n",
    "        # Find the start and end positions of the answer in the context\n",
    "        start_position = self.answer_starts[idx]\n",
    "        end_position = start_position + len(answer) - 1\n",
    "\n",
    "        answer_span = torch.zeros_like(q_encode[\"input_ids\"])\n",
    "        # print(answer_span.shape)\n",
    "\n",
    "\n",
    "        # align the positions\n",
    "        context_len = len(context)\n",
    "\n",
    "        start_token_idx = end_token_idx = None\n",
    "\n",
    "\n",
    "        if start_position <= context_len:\n",
    "            start_token_idx = len(self.tokenizer.tokenize(context[:start_position]))\n",
    "\n",
    "        if end_position <= context_len:\n",
    "            end_token_idx = len(self.tokenizer.tokenize(context[:end_position])) -1\n",
    "\n",
    "        if start_token_idx is not None:\n",
    "            if end_token_idx is not None:\n",
    "                answer_span[torch.arange(start_token_idx, end_token_idx)] = 1\n",
    "            else:\n",
    "                answer_span[torch.arange(start_token_idx, len(answer_span))] = 1\n",
    "        \n",
    "\n",
    "        return {\n",
    "            \"question_encoding\": q_encode,\n",
    "            \"context_encoding\": c_encode,\n",
    "            \"answer_span\": answer_span,\n",
    "            \"question_ids\": question_id\n",
    "        }\n",
    "\n",
    "\n",
    "data_folder = \"../data/rrc/laptop\"\n",
    "mini_batch = 4\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "contexts, questions, question_ids, answer_texts, start_positions = read_json_examples(os.path.join(data_folder, \"train.json\"))\n",
    "train_dataset = QADataset(questions, contexts, answer_texts, start_positions, tokenizer, question_ids)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=mini_batch)\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    print(batch[\"question_encoding\"])\n",
    "    print(batch[\"answer_span\"])\n",
    "    print(DualEncoder(\"bert-base-uncased\", 768)(batch[\"question_encoding\"], batch[\"context_encoding\"]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.arange(3, 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
